{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tatianaesc/livroescd/blob/main/livro_ESCD_feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llw41FPOuOfM"
      },
      "source": [
        "# Livro Engenharia de Software para Ciência de Dados (Ed. Casa do Código)\n",
        "Marcos Kalinowski, Tatiana Escovedo, Hugo Villamizar e Hélio Lopes\n",
        "\n",
        "### Exemplo Prático de Feature Selection em Python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta prática, estudar o processo de seleção de atributos, também conhecido como *feature selection*. Para tal, vamos trabalhar com o popular dataset Wine (disponível em https://archive.ics.uci.edu/ml/datasets/Wine), extraído a partir de uma análise química de vinhos cultivados na mesma região da Itália, mas derivados de três produtores diferentes. O objetivo deste dataset é identificar o produtor do vinho com base em 13 características químicas do vinho, ou seja, é um problema de Classificação. O dataset contém 178 instâncias (linhas), sendo 59 do produtor 1, 71 do do produtor 2 e 48 do produtor 3.\n",
        "\n",
        "Todo o código está comentado, para facilitar o entendimento. Iniciaremos esta prática importando os pacotes necessários para o notebook:"
      ],
      "metadata": {
        "id": "OdhL5lCwGoMy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBkM2DL9J3E6"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import set_printoptions\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# Novos imports\n",
        "from sklearn.feature_selection import SelectKBest # para a Seleção Univariada\n",
        "from sklearn.feature_selection import f_classif # para o teste ANOVA da Seleção Univariada\n",
        "from sklearn.feature_selection import RFE # para a Eliminação Recursiva de Atributos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCBbRV0TuFfg"
      },
      "source": [
        "Vamos então carregar o dataset, converter seus dados para um dataframe, adicionar a coluna target e exibir as primeiras linhas para checarmos se tudo foi carregado com sucesso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J4CfumWuCKx"
      },
      "source": [
        "# Carrega arquivo csv usando Pandas usando uma URL\n",
        "\n",
        "# Informa a URL de importação do dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "\n",
        "# Informa o cabeçalho das colunas\n",
        "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "\n",
        "# Lê o arquivo utilizando as colunas informadas\n",
        "dataset = pd.read_csv(url, names=colunas, skiprows=0, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48gUsAucKPp5"
      },
      "source": [
        "Em seguida, iremos preparar nossos dados para a aplicação dos métodos de seleção de atributos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRvsrgffKI1e"
      },
      "source": [
        "# separando os atributos e a classe do dataset\n",
        "array = dataset.values\n",
        "X = array[:,0:8]\n",
        "y = array[:,8]\n",
        "\n",
        "# A semente (seed) pode ser qualquer número, e garante que os resultados possam ser reproduzidos de forma idêntica toda vez que o script for rodado. \n",
        "# Isto é muito importante quando trabalhamos com modelos ou métodos que utilizam de algum tipo de aleatoriedade.\n",
        "seed = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfXVAOSNqexr"
      },
      "source": [
        "### Seleção Univariada\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
        "\n",
        "A função **SelectKBest()** pode ser usada com diversos testes estatísticos para selecionar os atributos. Vamos usar o teste *ANOVA F-value* e selecionar os 4 melhores atributos que podem ser usados como variáveis preditoras.\n",
        "\n",
        "Neste contexto, o teste *ANOVA F-value* estima o quanto cada característica de X é dependente da classe y, e é um teste especialmente adequado quanto temos a variável de saída categórica e as variáveis de entrada numéricas, como neste dataset.\n",
        "\n",
        "Para saber mais: https://www.statisticshowto.com/probability-and-statistics/f-statistic-value-test/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6zCh09WqYWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa350383-8601-486c-cd5d-42c983c76469"
      },
      "source": [
        "# Função para seleção de atributos\n",
        "best_var = SelectKBest(score_func=f_classif, k=4)\n",
        "\n",
        "# Executa a função de pontuação em (X, y) e obtém os atributos selecionados\n",
        "fit = best_var.fit(X, y)\n",
        "\n",
        "# Reduz X para os atributos selecionados\n",
        "features = fit.transform(X)\n",
        "\n",
        "# Resultados\n",
        "print('\\nNúmero original de atributos:', X.shape[1])\n",
        "print('\\nNúmero reduzido de atributos:', features.shape[1])\n",
        "\n",
        "# Exibe os atributos orginais\n",
        "print(\"\\nAtributos Originais:\", dataset.columns[0:8])\n",
        "\n",
        "# Exibe as pontuações de cada atributos e os 4 escolhidas (com as pontuações mais altas): preg, plas, mass e age.\n",
        "set_printoptions(precision=3) # 3 casas decimais\n",
        "print(\"\\nScores dos Atributos Originais:\", fit.scores_)\n",
        "print(\"\\nAtributos Selecionados:\", best_var.get_feature_names_out(input_features=dataset.columns[0:8]))\n",
        "\n",
        "# Imprime o dataset apenas com as colunas selecionadas\n",
        "print(\"\\n\", features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Número original de atributos: 8\n",
            "\n",
            "Número reduzido de atributos: 4\n",
            "\n",
            "Atributos Originais: Index(['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age'], dtype='object')\n",
            "\n",
            "Scores dos Atributos Originais: [ 39.67  213.162   3.257   4.304  13.281  71.772  23.871  46.141]\n",
            "\n",
            "Atributos Selecionados: ['preg' 'plas' 'mass' 'age']\n",
            "\n",
            " [[  6.  148.   33.6  50. ]\n",
            " [  1.   85.   26.6  31. ]\n",
            " [  8.  183.   23.3  32. ]\n",
            " ...\n",
            " [  5.  121.   26.2  30. ]\n",
            " [  1.  126.   30.1  47. ]\n",
            " [  1.   93.   30.4  23. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSrheF65qu_Q"
      },
      "source": [
        "### Eliminação Recursiva de Atributos\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
        "\n",
        "Iremos aplicar a técnica de eliminação recursiva de atributos com um algoritmo de Regressão Logística (poderia ser qualquer classificador) para selecionar as 4 melhores variáveis preditoras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tRYazhXqvK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6abbce0-95bc-4f53-e943-dbe8a34c8d2d"
      },
      "source": [
        "# Criação do modelo\n",
        "modelo = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Eliminação Recursiva de Variáveis\n",
        "rfe = RFE(modelo, n_features_to_select=4)\n",
        "fit = rfe.fit(X, y)\n",
        "\n",
        "# Print dos resultados\n",
        "print(\"Atributos Originais:\", dataset.columns[0:8])\n",
        "\n",
        "# Exibe os atributos selecionados (marcados como True em \"Atributos Selecionados\" \n",
        "# e com valor 1 em \"Ranking dos Atributos\"): preg, plas, mass e pedi.\n",
        "# (Basta mapear manualmente o índice dos nomes dos respectivos atributos)\n",
        "print(\"\\nAtributos Selecionados: %s\" % fit.support_)\n",
        "print(\"\\nRanking de atributos: %s\" % fit.ranking_)\n",
        "print(\"\\nQtd de melhores Atributos: %d\" % fit.n_features_)\n",
        "print(\"\\nNomes dos Atributos Selecionados: %s\" % fit.get_feature_names_out(input_features=dataset.columns[0:8]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Atributos Originais: Index(['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age'], dtype='object')\n",
            "\n",
            "Atributos Selecionados: [ True  True False False False  True  True False]\n",
            "\n",
            "Ranking de atributos: [1 1 3 5 4 1 1 2]\n",
            "\n",
            "Qtd de melhores Atributos: 4\n",
            "\n",
            "Nomes dos Atributos Selecionados: ['preg' 'plas' 'mass' 'pedi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpqpF6Rs2BsE"
      },
      "source": [
        "### Importância de Atributos com ExtraTrees\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
        "\n",
        "Iremos construir um classificador ExtraTreesClassifier para calcular a importância dos atributos e posteriormente, selecionar quais iremos utilizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1TMFLjU2BxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f313e29e-1d0d-459a-d7b3-a060a56ad939"
      },
      "source": [
        "# Criação do modelo para seleção de atributos\n",
        "modelo = ExtraTreesClassifier(n_estimators=100)\n",
        "modelo.fit(X,y)\n",
        "\n",
        "# Exibe os atributos orginais\n",
        "print(\"\\nAtributos Originais:\", dataset.columns[0:8])\n",
        "\n",
        "# Exibe a pontuação de importância para cada atributo (quanto maior a pontuação, mais importante é o atributo). \n",
        "# Atributos selecionados: plas, mass, pedi, age.\n",
        "print(modelo.feature_importances_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Atributos Originais: Index(['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age'], dtype='object')\n",
            "[0.109 0.231 0.1   0.081 0.073 0.14  0.122 0.144]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resumo dos atributos selecionados:\n",
        "\n",
        "*   Técnica 1: preg, plas, mass e age\n",
        "*   Técnica 2: preg, plas, mass e pedi\n",
        "*   Técnica 3: plas, mass, pedi, age"
      ],
      "metadata": {
        "id": "GMuqIJ4uq4d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBS: É possível utilizar técnicas de *feature selection* como parte de um pipeline. Veja um exemplo no trecho de código a seguir:"
      ],
      "metadata": {
        "id": "Z1WbgCXS_Cun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.pipeline import Pipeline \n",
        "\n",
        "pipeline = Pipeline([\n",
        "  ('feature_selection', SelectKBest(score_func=f_classif, k=4)),\n",
        "  ('classification', RandomForestClassifier(n_estimators=100, max_features=3, random_state=7))\n",
        "])"
      ],
      "metadata": {
        "id": "hoadbYNo_Sek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando técnicas de *feature selection*, podemos decidir quais queremos selecionar para que sejam utilizados no treinamento do modelo de Machine Learning, o que seria o passo seguinte.\n",
        "\n",
        "Nesta prática, vimos como utilizar três diferentes técnicas de feature selection. Note que, dependendo da técnica utilizada, os atributos selecionados poderão ser diferentes."
      ],
      "metadata": {
        "id": "1mPJY7tEHoeX"
      }
    }
  ]
}